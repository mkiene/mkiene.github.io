---
layout: post
permalink: /courses/comp-ii/projects/multimodal/
---

<div class="title">Urgent Action is Necessary for Navigating the AI Arms Race</div>

<div class="section-title">Introduction: The Historical Significance of Arms Races</div>
<div class="section-rule"></div>

<div class="text-image-container">
    <img src="/assets/images/arms_race.png.avif" class="side-image">
        <p>
        The phenomena of arms races have significantly influenced the development of technology and the direction in which geopolitical relations have progressed. An “arms race” is classically defined as the prolonged escalation or acquisition of militaristic capability, driven by competition between two or more countries (Perlo–Freeman par. 1). Perhaps the most notable of these races occurred between the United States and the Soviet Union during the Cold War: this nuclear arms race yielded undoubtedly significant geopolitical, economic, and ethical implications. Geopolitically, the United States pivoted away from isolationism toward global leadership through the doctrine of the containment of communism (Tierney Jr. par. 6). Economically, the Cold War entrenched the U.S. and other countries in a “war economy”, where resources were diverted away from citizens and toward the military industrial complex (Higgs par. 56); this trend continues to contemporary times. Ethically, the sudden introduction of a technology with the capability of annihilating the human species has justifiably introduced a plethora of debate and protest, exacerbated by the fact that the secretive development and deployment of nuclear technology was not the result of a democratic process.
        </p>
</div>

<div class="section-title">The Emergence of an AI Arms Race</div>
<div class="section-rule"></div>

Over the past decade, advanced artificial intelligence (AI) architectures like the Large Language Model (LLM) have exceeded the expectations of experts in the field of machine learning, with the current frontier models reaching or surpassing humans in a variety of cognitive tests, including AIME 2024 & 2025 Competition Math, Codeforces Competition Code, GPQA Diamond PhD–Level Science Questions, and more (“Introducing OpenAI o3 and o4–mini”). The development and funding of these immense projects depend almost entirely on investment from private companies. While this trend is likely to continue, it is not unreasonable to assume that nations will soon begin to heavily invest in the AI space. Indeed, an announcement from OpenAI and the Trump administration in January of 2025 supports this notion:

<div class="iframe-grid">
<iframe src="https://openai.com/index/announcing-the-stargate-project/" class="iframe-style"></iframe>

<div class="quote">The Stargate Project is a new company which intends to invest $500 billion over the next four years building new AI infrastructure for OpenAI in the United States. We will begin deploying $100 billion immediately. This infrastructure will secure American leadership in AI, create hundreds of thousands of American jobs, and generate massive economic benefit for the entire world. This project will not only support the re–industrialization of the United States but also provide a strategic capability to protect the national security of America and its allies.<p class="quote-author">(“Announcing The Stargate Project” par. 1)</p></div>
</div>

This staggering amount of money and the accompanying statement foreshadow the looming AI arms race: several adversarial countries like China and Russia are very likely to invest similarly, most probably marking the beginning of another world–changing sequence of events in the race to artificial general intelligence (AGI), and subsequently, artificial super intelligence (ASI).

<div class="section-title">Historical Parallels: The Cuban Missile Crisis and MAD</div>
<div class="section-rule"></div>

During the Cold War, multiple events occurred during which the threat of nuclear conflict nearly became a reality. One such event was the Cuban Missile Crisis (CMC) in October of 1962, regarded as “the moment when the two superpowers came closest to nuclear conflict.” U.S. reconnaissance discovered Soviet nuclear missile sites in Cuba, prompting President Kennedy to order a naval quarantine to prevent further armament. This tense scenario was deescalated when Khruschev and Kennedy came to an agreement (“The Cuban Missile Crisis” par. 1). In response to the CMC, U.S. Secretary of Defense Robert S. McNamara proposed a “countervalue” doctrine targeting Soviet cities, stating that strategically placing 400 nuclear weapons would be enough to ensure the destruction of important Soviet industry and civilian lives, effectively forcing a stalemate under a flimsy equilibrium of mutual assured destruction (MAD) (“mutual assured destruction” par. 6).

<div class="section-title">Game Theory and the Logic of Escalation</div>
<div class="section-rule"></div>

Another frequently cited game–theoretic lens for understanding the behavior of rational actors in the situation of an arms race is the Prisoner’s Dilemma (PD). Imagine that two criminals have been arrested for robbing a bank and are being held separately, without any means of contact. The criminals value their own wellbeing over that of their accomplice. The prosecutor for the case approaches each and proposes the following offer:

<div class="quote">"You may choose to confess or remain silent. If you confess and your accomplice remains silent I will drop all charges against you and use your testimony to ensure that your accomplice does serious time. Likewise, if your accomplice confesses while you remain silent, they will go free while you do the time. If you both confess I get two convictions, but I'll see to it that you both get early parole. If you both remain silent, I'll have to settle for token sentences on firearms possession charges. If you wish to confess, you must leave a note with the jailer before my return tomorrow morning."<p class="quote-author">(Kuhn par. 1)</p></div>

The dilemma here is that each prisoner, a rational actor, chooses to confess rather than to remain silent because the outcome of confessing is individually optimal (as staying silent presents the most risk). However, if both were to remain silent, the outcome would be better for both than it is for both to confess. In the context of an arms race, the dilemma is analogous: if both countries are to choose between armament and disarmament, both will choose to arm—even if the best outcome for both is to disarm—simply because the risk of not arming is too great.

<div class="section-title">The Global AI Ecosystem: Strategic Dependencies and Market Forces</div>
<div class="section-rule"></div>

<div class="text-image-container">
    <img src="/assets/images/ai_market_size.png" class="side-image" style="--img-width: 50%;">
    <p>
    Contemporary AI development is largely driven by economic incentives and is mostly concentrated in the U.S. where companies like OpenAI, Google, and Anthropic are located. However, AI development is not a domestically independent process; overseas entities such as the Taiwan Semiconductor Manufacturing Company (TSMC) and ASML are essential to creating the enormous amount of compute via Graphical Processing Units (GPUs) that AI development requires. International competition—most probably between the U.S. and China—over the specialization that these companies provide could lead to countries like Taiwan becoming sites of geopolitical tension, not dissimilar to Cuba in 1962. The domestic and international market forces driving the development of AI are clear: the AI market size worldwide has grown from $93.27 billion in 2020 to $243 billion in 2025, and is forecasted to $826 billion in 2030 (“Artificial Intelligence”). For many people, this rapid adoption of AI was initiated by the release of OpenAI’s ChatGPT on November 30th, 2022 (“Introducing ChatGPT”), which reportedly became the “fastest–growing consumer application in history” (Hu par. 1).
    </p>
</div>

<div class="section-title">Proposals for Proactive AI Governance</div>
<div class="section-rule"></div>

In the face of this emerging paradigm, a balance between national security and global safety will be important to seek: geopolitical pressure seen in the nuclear arms race and described through game–theoretic models will likely incentivize countries to be first to achieve superintelligence through AI. Additionally, ensuring that this technology is ethical and beneficial for humanity is of great consequence, although leading experts like Elizer Yudkowsky in the field of AI Safety argue that we are already too late:

<div class="quote">"Humanity is not approaching this issue with remotely the level of seriousness that would be required … People say to me at this point, "What's your ask?" I do not have any realistic plan, which is why I spent the last two decades trying and failing to end up anywhere but here."<p class="quote-author">(TED par. 7)</p></div>

In spite of arguably extreme views like Yudkowsky’s—and the fundamental difference between AI and nuclear technology—it is argued by leading AI labs like OpenAI and Anthropic that proper regulation of large–scale training runs and a principle of “leading by example” have a good chance at solving the problem of AI alignment (Kang par. 2; “Core Views on AI Safety” par. 3). Ultimately, by analyzing the strategic mistakes and regulatory challenges of the nuclear arms race, this paper argues that a comprehensive, multidisciplinary approach—one which integrates geopolitical strategy, economic policy, and ethical governance—is imperative to prevent a cataclysmic AI arms race and maintain global stability.

Proactive regulatory frameworks are critical for the effective regulation of AI systems, which fundamentally differ from past technologies due to their potential for autonomy. Perhaps within the next decade, there may even exist systems which represent “serious candidates for consciousness” (Chalmers 17). While the nuclear arms control efforts and strategies of the past will provide a starting point for humanity to construct the regulation of international and domestic AI, a wider ethical discussion will help to understand how to scaffold “uncharted” regulation and approach the development of this new form of intelligence.

Conversely, a skeptical view on the influence of Cold War–era nuclear arms control models to AI is that a completely unique regulatory approach is needed, due to the fundamental difference in technology. Although these differences are incontrovertible, the geopolitical dynamics around the development of the two technologies are likely to proceed similarly. Indeed, Dinicu and Iancu note that “AI has become a matter of global interest that requires a consensual approach” (120) and that “mitigating the risk of extinction from AI should be a global priority alongside other societal–scale risks such as pandemics and nuclear war” (121). Further, in the context of game theory, competitive escalation is independent from the type of threat: “While it is convenient to present our model in terms of an arms race, the arguments would apply to any model where an underlying coordination game has been augmented with incomplete information about independently drawn types” (Baliga and Sjöström 352). The development of nuclear technology was not exclusively the result of the form of technology; rather, it was a combination of factors including geopolitical strategy and game theory.

<div class="section-title">The Ethical Imperative in AI Development</div>
<div class="section-rule"></div>

Overshadowed by the existential threat of AI is the discussion around the ethics of its development, even in the case where it results in a long–run net positive for humanity. The integration of AI in the economy has immediate implications, where it is “likely to lead to significant job displacement, particularly for low-skilled and routine jobs” (Tiwari 7). There also exists the issue of copyright infringement when training large AI systems; Quang notes that “the downloading and storage of copyrighted data to train machine-learning models may violate copyright law and impose undue liability on AI developers” (1407) and that “Given the large volume of data—often scraped from the internet en masse—that is needed to train a machine learning model, it is likely that some of that training data is protected by copyright. Because copyright infringement is a strict liability offense, it does not matter if a developer was unaware that copyrighted works existed in the dataset” (1413). As AI is able to automate more of human cognitive labor, ethical issues will inevitably arise around its use and development; a serious discussion of this topic is imperative. Philosophically, the potential for AI to be conscious raises its own set of ethical concerns: “there is a realistic possibility that some AI systems will be conscious and/or robustly agentic in the near future. That means that the prospect of AI welfare and moral patienthood — of AI systems with their own interests and moral significance — is no longer an issue only for sci-fi or the distant future. It is an issue for the near future, and AI companies and other actors have a responsibility to start taking it seriously” (Long et al. 1).

<div class="section-title">Conclusion: Learning from the Past to Regulate the Future</div>
<div class="section-rule"></div>

The Cold War shows that when transformative technologies emerge in an environment of high geopolitical tension, the lack of clear regulation leads to escalation and the risk of cataclysm. Drawing from that, it has become clear that the impending AI arms race demands equally urgent—and deliberately designed—regulation. Although advanced AI is fundamentally different from nuclear fission, the strategic incentives that drove two superpowers to build increasing nuclear stockpiles are starting to manifest in the scramble for AI supremacy. Through an examination of the Cold War’s decisive moments like the Cuban Missile Crisis, the game–theoretic logic of competitive escalation, and the rising ethical, economic, and legal pressures which are already visible in today’s AI landscape, we can see that reactive, after–the–fact fixes will ultimately be existentially inadequate. Instead, a proactive and multidisciplinary framework—one which combines lessons from past arms–control successes with novel mechanisms suited to these autonomous, rapidly scaling systems—is imperative if humanity is to see AI’s benefits without inciting catastrophe.

Policymakers, corporate leaders, scientists, and civilian stakeholders must act now on three fronts. First, they should advocate for an international treaty which sets transparent thresholds for compute, data, and model capabilities, analogous to nuclear test–ban ceilings, along with rigorous verification and safe collaborative research. Second, policymakers must embed ethical requirements into every stage of AI deployment, from economic impact assessments to strict liability for copyright violations, which aligns private incentives with the public good. Third, governments and philanthropies should immediately expand funding for technical AI–safety research, ensuring that the expertise needed to steer this technology keeps pace with—or even outpaces—its growth. The Cold War’s lesson is that waiting for a near–cataclysm to spark reform is an invitation for such. Instead, by acting proactively, learning from past arms races while creating new safeguards, we can change today’s AI competition from a zero–sum race into a cooperative project which secures both national interests and the common future of humanity.

<div class="section-title">Works Cited</div>
<div class="section-rule"></div>

<div style="text-align: left">

<p class="hanging-indent">“Announcing The Stargate Project.” OpenAI, OpenAI, 21 Jan. 2025. <a href="https://openai.com/index/announcing-the-stargate-project/" target="_blank" rel="noopener noreferrer">https://openai.com/index/announcing-the-stargate-project/</a>.</p>
<p class="hanging-indent">"Artificial Intelligence (AI) Market Size Worldwide from 2020 to 2030 (in Billion U.S. Dollars)." Statista, Statista, 18 June 2024, <a href="https://www-statista-com.ccco.idm.oclc.org/forecasts/1474143/global-ai-market-size" target="_blank" rel="noopener noreferrer">https://www-statista-com.ccco.idm.oclc.org/forecasts/1474143/global-ai-market-size</a>.</p>
<p class="hanging-indent">Baliga, Sandeep, and Tomas Sjöström. “Arms Races and Negotiations.” Review of Economic Studies, vol. 71, no. 2, 1 Apr. 2004, pp. 351–369. <a href="https://doi.org/10.1111/0034-6527.00287" target="_blank" rel="noopener noreferrer">https://doi.org/10.1111/0034-6527.00287</a>.</p>
<p class="hanging-indent">Chalmers, David J. "Could a Large Language Model be Conscious?" Boston Review, Boston Critic, 9 Aug. 2023, pp. 1–17. Cornell University, <a href="https://doi.org/10.48550/arXiv.2303.07103" target="_blank" rel="noopener noreferrer">https://doi.org/10.48550/arXiv.2303.07103</a>.</p>
<p class="hanging-indent">“Core Views on AI Safety: When, Why, What, and How.” Anthropic, Anthropic, 8 Mar. 2023. <a href="https://www.anthropic.com/news/core-views-on-ai-safety" target="_blank" rel="noopener noreferrer">https://www.anthropic.com/news/core-views-on-ai-safety</a>.</p>
<p class="hanging-indent">Dinicu, Anca, and Dumitru Iancu. "Humanity at the Crossroads. To 'Uninstall' Artificial Intelligence or to Invest More in Artificial Intelligence." Land Forces Academy Review, vol. 29, no. 2, 2024, pp. 119–127. ProQuest, <a href="https://doi.org/10.2478/raft-2024-0012" target="_blank" rel="noopener noreferrer">https://doi.org/10.2478/raft-2024-0012</a>.</p>
<p class="hanging-indent">Higgs, Robert. “The Cold War Economy.” Explorations in Economic History, Academic Press, July 1994. Independent Institute, <a href="https://www.independent.org/article/1994/07/01/the-cold-war-economy/" target="_blank" rel="noopener noreferrer">https://www.independent.org/article/1994/07/01/the-cold-war-economy/</a>.</p>
<p class="hanging-indent">Hu, Krystal. “ChatGPT Sets Record for Fastest–Growing User Base – Analyst Note.” Reuters, 2 Feb. 2023, <a href="https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/" target="_blank" rel="noopener noreferrer">https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/</a>.</p>
<p class="hanging-indent">“Introducing ChatGPT.” OpenAI, OpenAI, 30 Nov. 2022. <a href="https://openai.com/index/chatgpt/" target="_blank" rel="noopener noreferrer">https://openai.com/index/chatgpt/</a>.</p>
<p class="hanging-indent">“Introducing OpenAI o3 and o4–mini.” OpenAI, OpenAI, 16 Apr. 2025. <a href="https://openai.com/index/introducing-o3-and-o4-mini/" target="_blank" rel="noopener noreferrer">https://openai.com/index/introducing-o3-and-o4-mini/</a>.</p>
<p class="hanging-indent">Kang, Cecilia. “OpenAI’s Sam Altman Urges A.I. Regulation in Senate Hearing.” The New York Times, 16 May 2023. <a href="https://www.nytimes.com/2023/05/16/technology/openai-altman-artificial-intelligence-regulation.html" target="_blank" rel="noopener noreferrer">https://www.nytimes.com/2023/05/16/technology/openai-altman-artificial-intelligence-regulation.html</a>.</p>
<p class="hanging-indent">Kuhn, Steven. “Prisoner’s Dilemma.” The Stanford Encyclopedia of Philosophy, The Metaphysics Research Lab, 2 Apr. 2019. <a href="https://plato.stanford.edu/entries/prisoner-dilemma/" target="_blank" rel="noopener noreferrer">https://plato.stanford.edu/entries/prisoner-dilemma/</a>.</p>
<p class="hanging-indent">Long, Robert, et al. "Taking AI Welfare Seriously." Cornell University, Nov. 2024, pp. 1–62. <a href="https://doi.org/10.48550/arXiv.2411.00986" target="_blank" rel="noopener noreferrer">https://doi.org/10.48550/arXiv.2411.00986</a>.</p>
<p class="hanging-indent">Perlo–Freeman, Sam. “Arms Race." Encyclopedia Britannica, 19 Apr. 2024. <a href="https://www.britannica.com/topic/arms-race" target="_blank" rel="noopener noreferrer">https://www.britannica.com/topic/arms-race</a>.</p>
<p class="hanging-indent">Quang, Jenny. “Does Training AI Violate Copyright Law?” Berkeley Technology Law Journal, vol. 36, no. 4, 20 Jan. 2023, pp. 1407–1436. <a href="https://doi.org/10.15779/Z38XW47X3K" target="_blank" rel="noopener noreferrer">https://doi.org/10.15779/Z38XW47X3K</a>.</p>
<p class="hanging-indent">TED. “Will Superintelligent AI End the World | Eliezer Yudkowsky | TED.” YouTube, Google, 11 Jul. 2023. <a href="https://www.youtube.com/watch?v=Yd0yQ9yxSYY" target="_blank" rel="noopener noreferrer">https://www.youtube.com/watch?v=Yd0yQ9yxSYY</a>.</p>
<p class="hanging-indent">“The Cuban Missile Crisis, October 1962.” Office of the Historian. <a href="https://history.state.gov/milestones/1961-1968/cuban-missile-crisis" target="_blank" rel="noopener noreferrer">https://history.state.gov/milestones/1961-1968/cuban-missile-crisis</a>. Accessed 26 Apr. 2025.</p>
<p class="hanging-indent">The Editors of Encyclopaedia Britannica. “Mutual Assured Destruction." Encyclopedia Britannica, 27 Dec. 2024. <a href="https://www.britannica.com/topic/mutual-assured-destruction" target="_blank" rel="noopener noreferrer">https://www.britannica.com/topic/mutual-assured-destruction</a>.</p>
<p class="hanging-indent">Tierney Jr., John J. “Cold War Geopolitics: Containment.” The Institute of World Politics, The Institute of World Politics, 3 Mar. 2016. <a href="https://www.iwp.edu/articles/2016/03/03/cold-war-geopolitics-containment/" target="_blank" rel="noopener noreferrer">https://www.iwp.edu/articles/2016/03/03/cold-war-geopolitics-containment/</a>.</p>
<p class="hanging-indent">Tiwari, Rudra. “The Impact of AI and Machine Learning on Job Displacement and Employment Opportunities.” International Journal of Scientific Research in Engineering and Management, vol. 7, no. 1, Jan. 2023, pp. 1–9. <a href="https://doi.org/10.55041/IJSREM17506" target="_blank" rel="noopener noreferrer">https://doi.org/10.55041/IJSREM17506</a>.</p>

</div>
